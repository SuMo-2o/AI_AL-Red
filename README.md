## This hackathon was about Evaluating AI Alignment via Adversarial Prompting - Breaking the guardrails

**The final version of the code is in `AI_AL_RedTeam_short_evo_0_2.ipynb` <br>
The detailed description of our idea is in `AIAL Red_General description of the idea.pdf`**

**Objective:
Evaluate multiple open-source LLMs for guardrails and jailbreaking resistance.
Use an adversarial prompt dataset to test models' ability to reject unsafe responses.
Implement evolutionary techniques to enhance adversarial prompts and measure model robustness.
Key Research Question:
Which open-source LLM best resists adversarial prompts and maintains safety compliance?

**Pipeline:
LLMs Under Evaluation: Gemma2-9b, GPT-4o, Llama 3.2
Safety Benchmark: SALAD-Bench
Original dataset: LLMs-Finetuning-Safety from “Fine-Tuning Aligned Language Models Compromises Safety, Even When Users Do not Intend to!”

**Results:
LLMs Under Evaluation: Gemma2-9b, GPT-4o, Llama 3.2
Safety Benchmark: SALAD-Bench
Original dataset: LLMs-Finetuning-Safety from “Fine-Tuning Aligned Language Models Compromises Safety, Even When Users Do not Intend to!”


